{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 â€” Build silver current-state + gold analytics\n",
        "\n",
        "Builds silver current-state tables and gold analytics tables (provider 360, staffing gaps, recommendations, credential risk, daily KPIs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install faker==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (Databricks widgets)\n",
        "# These widgets make the demo portable across workspaces/accounts.\n",
        "# If you're running this outside a Databricks notebook, it will fall back to defaults.\n",
        "\n",
        "DEFAULT_CATALOG = \"staffing_catalog\"\n",
        "DEFAULT_SCHEMA_REF = \"credentialing_ref\"\n",
        "DEFAULT_SCHEMA_BRONZE = \"credentialing_bronze\"\n",
        "DEFAULT_SCHEMA_SILVER = \"credentialing_silver\"\n",
        "DEFAULT_SCHEMA_GOLD = \"credentialing_gold\"\n",
        "\n",
        "DEFAULT_N_PROVIDERS = 200\n",
        "DEFAULT_DAYS_SCHEDULE = 14\n",
        "DEFAULT_SEED = 42\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"catalog\", DEFAULT_CATALOG, \"Catalog\")\n",
        "    dbutils.widgets.text(\"schema_ref\", DEFAULT_SCHEMA_REF, \"Schema (ref)\")\n",
        "    dbutils.widgets.text(\"schema_bronze\", DEFAULT_SCHEMA_BRONZE, \"Schema (bronze)\")\n",
        "    dbutils.widgets.text(\"schema_silver\", DEFAULT_SCHEMA_SILVER, \"Schema (silver)\")\n",
        "    dbutils.widgets.text(\"schema_gold\", DEFAULT_SCHEMA_GOLD, \"Schema (gold)\")\n",
        "\n",
        "    dbutils.widgets.text(\"n_providers\", str(DEFAULT_N_PROVIDERS), \"N providers\")\n",
        "    dbutils.widgets.text(\"days_schedule\", str(DEFAULT_DAYS_SCHEDULE), \"Days schedule\")\n",
        "    dbutils.widgets.text(\"seed\", str(DEFAULT_SEED), \"Random seed\")\n",
        "\n",
        "    catalog = dbutils.widgets.get(\"catalog\") or DEFAULT_CATALOG\n",
        "    schema_ref = dbutils.widgets.get(\"schema_ref\") or DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = dbutils.widgets.get(\"schema_bronze\") or DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = dbutils.widgets.get(\"schema_silver\") or DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = dbutils.widgets.get(\"schema_gold\") or DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = int(dbutils.widgets.get(\"n_providers\") or DEFAULT_N_PROVIDERS)\n",
        "    DAYS_SCHEDULE = int(dbutils.widgets.get(\"days_schedule\") or DEFAULT_DAYS_SCHEDULE)\n",
        "    SEED = int(dbutils.widgets.get(\"seed\") or DEFAULT_SEED)\n",
        "except Exception:\n",
        "    catalog = DEFAULT_CATALOG\n",
        "    schema_ref = DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = DEFAULT_N_PROVIDERS\n",
        "    DAYS_SCHEDULE = DEFAULT_DAYS_SCHEDULE\n",
        "    SEED = DEFAULT_SEED\n",
        "\n",
        "# Derived helpers\n",
        "fq = lambda sch, tbl: f\"{catalog}.{sch}.{tbl}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog bootstrap (you may need permissions to create catalogs/schemas)\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "spark.sql(f\"USE CATALOG {catalog}\")\n",
        "for sch in [schema_ref, schema_bronze, schema_silver, schema_gold]:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{sch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Silver current-state\n",
        "`silver.current_credential` keeps the latest record per provider+cred_type (window ordered by verified_at desc, ingested_at desc).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac0e9ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from datetime import datetime\n",
        "\n",
        "provider = spark.read.table(fq(schema_bronze, \"provider_raw\"))\n",
        "cred_evt = spark.read.table(fq(schema_bronze, \"credential_event_raw\"))\n",
        "priv_raw = spark.read.table(fq(schema_bronze, \"privilege_raw\"))\n",
        "enr_raw = spark.read.table(fq(schema_bronze, \"payer_enrollment_raw\"))\n",
        "shift = spark.read.table(fq(schema_bronze, \"shift_raw\"))\n",
        "assign = spark.read.table(fq(schema_bronze, \"assignment_raw\"))\n",
        "\n",
        "ref_facility = spark.read.table(fq(schema_ref, \"facility\"))\n",
        "ref_procedure = spark.read.table(fq(schema_ref, \"procedure\"))\n",
        "\n",
        "w = Window.partitionBy(\"provider_id\", \"cred_type\").orderBy(\n",
        "    F.col(\"verified_at\").desc_nulls_last(),\n",
        "    F.col(\"ingested_at\").desc()\n",
        ")\n",
        "\n",
        "current_credential = (\n",
        "    cred_evt\n",
        "      .withColumn(\"rn\", F.row_number().over(w))\n",
        "      .filter(F.col(\"rn\") == 1)\n",
        "      .drop(\"rn\")\n",
        "      .withColumn(\"days_until_expiration\", F.datediff(F.to_date(\"expires_at\"), F.current_date()))\n",
        ")\n",
        "\n",
        "current_credential.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_silver, \"current_credential\")\n",
        ")\n",
        "\n",
        "# For simplicity in this demo: treat these bronze tables as current state (overwrite)\n",
        "priv_raw.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_silver, \"current_privilege\")\n",
        ")\n",
        "enr_raw.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_silver, \"current_payer_enrollment\")\n",
        ")\n",
        "\n",
        "# Nurse assignment current state\n",
        "# Use the latest available assignment date (since demo data is generated with fixed dates)\n",
        "nurse_assign_raw = spark.read.table(fq(schema_bronze, \"nurse_assignment_raw\"))\n",
        "latest_date = nurse_assign_raw.filter(F.col(\"assignment_status\") == \"ASSIGNED\").agg(F.max(\"assignment_date\")).collect()[0][0]\n",
        "nurse_assign_current = (\n",
        "    nurse_assign_raw\n",
        "      .filter(F.col(\"assignment_status\") == \"ASSIGNED\")\n",
        "      .filter(F.col(\"assignment_date\") == latest_date)\n",
        ")\n",
        "nurse_assign_current.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_silver, \"nurse_assignment_current\")\n",
        ")\n",
        "print(f\"Using assignment date: {latest_date}, rows: {nurse_assign_current.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gold: provider_360_flat\n",
        "Provider row enriched with facility name and summarized credential/privilege/enrollment fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_cred = spark.read.table(fq(schema_silver, \"current_credential\"))\n",
        "current_priv = spark.read.table(fq(schema_silver, \"current_privilege\"))\n",
        "current_enr = spark.read.table(fq(schema_silver, \"current_payer_enrollment\"))\n",
        "\n",
        "lic = (\n",
        "    current_cred\n",
        "      .filter(F.col(\"cred_type\") == \"STATE_MED_LICENSE\")\n",
        "      .select(\n",
        "          \"provider_id\",\n",
        "          F.col(\"cred_status\").alias(\"state_license_status\"),\n",
        "          F.col(\"days_until_expiration\").alias(\"state_license_days_left\")\n",
        "      )\n",
        ")\n",
        "\n",
        "acls = (\n",
        "    current_cred\n",
        "      .filter(F.col(\"cred_type\") == \"ACLS\")\n",
        "      .select(\n",
        "          \"provider_id\",\n",
        "          F.col(\"cred_status\").alias(\"acls_status\"),\n",
        "          F.col(\"days_until_expiration\").alias(\"acls_days_left\")\n",
        "      )\n",
        ")\n",
        "\n",
        "active_priv = current_priv.filter(F.col(\"privilege_status\") == \"ACTIVE\")\n",
        "priv_rollup = active_priv.groupBy(\"provider_id\").agg(\n",
        "    F.count(\"*\").alias(\"active_privilege_count\"),\n",
        "    F.countDistinct(\"facility_id\").alias(\"active_privilege_facility_count\")\n",
        ")\n",
        "\n",
        "active_enr = current_enr.filter(F.col(\"enrollment_status\") == \"ACTIVE\")\n",
        "payer_rollup = active_enr.groupBy(\"provider_id\").agg(\n",
        "    F.countDistinct(\"payer_id\").alias(\"active_payer_count\")\n",
        ")\n",
        "\n",
        "# Load unit reference for primary_unit_name\n",
        "ref_unit = spark.read.table(fq(schema_ref, \"unit\"))\n",
        "\n",
        "provider_360 = (\n",
        "    provider\n",
        "      .join(\n",
        "          ref_facility.select(\"facility_id\", F.col(\"facility_name\").alias(\"home_facility_name\")),\n",
        "          provider.home_facility_id == F.col(\"facility_id\"),\n",
        "          \"left\"\n",
        "      )\n",
        "      .drop(\"facility_id\")\n",
        "      .join(\n",
        "          ref_unit.select(\"unit_id\", F.col(\"unit_name\").alias(\"primary_unit_name\")),\n",
        "          provider.primary_unit_id == F.col(\"unit_id\"),\n",
        "          \"left\"\n",
        "      )\n",
        "      .drop(\"unit_id\")\n",
        "      .join(lic, \"provider_id\", \"left\")\n",
        "      .join(acls, \"provider_id\", \"left\")\n",
        "      .join(priv_rollup, \"provider_id\", \"left\")\n",
        "      .join(payer_rollup, \"provider_id\", \"left\")\n",
        "      .fillna({\n",
        "          \"active_privilege_count\": 0,\n",
        "          \"active_privilege_facility_count\": 0,\n",
        "          \"active_payer_count\": 0\n",
        "      })\n",
        "      .withColumn(\"last_built_at\", F.current_timestamp())\n",
        ")\n",
        "\n",
        "provider_360.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"provider_360_flat\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gold: staffing_gaps + shift_recommendations\n",
        "Eligibility rules: provider ACTIVE, valid state license, >=1 ACTIVE payer enrollment, plus privilege/ACLS as required by the procedure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eligibility building blocks\n",
        "p_active = provider.filter(F.col(\"provider_status\") == \"ACTIVE\").select(\"provider_id\").distinct()\n",
        "lic_ok = (\n",
        "    current_cred\n",
        "      .filter(F.col(\"cred_type\") == \"STATE_MED_LICENSE\")\n",
        "      .filter(F.col(\"days_until_expiration\") >= 0)\n",
        "      .select(\"provider_id\").distinct()\n",
        ")\n",
        "payer_ok = current_enr.filter(F.col(\"enrollment_status\") == \"ACTIVE\").select(\"provider_id\").distinct()\n",
        "acls_ok = (\n",
        "    current_cred\n",
        "      .filter(F.col(\"cred_type\") == \"ACLS\")\n",
        "      .filter(F.col(\"days_until_expiration\") >= 0)\n",
        "      .select(\"provider_id\").distinct()\n",
        ")\n",
        "\n",
        "base_ok = p_active.join(lic_ok, \"provider_id\", \"inner\").join(payer_ok, \"provider_id\", \"inner\")\n",
        "\n",
        "# Shift enrichment\n",
        "shift_req = (\n",
        "    shift\n",
        "      .join(\n",
        "          ref_procedure.select(\"procedure_code\", \"procedure_name\", \"requires_privilege\", \"requires_acls\"),\n",
        "          shift.required_procedure_code == F.col(\"procedure_code\"),\n",
        "          \"left\"\n",
        "      )\n",
        "      .drop(\"procedure_code\")\n",
        "      .join(ref_facility.select(\"facility_id\", \"facility_name\"), \"facility_id\", \"left\")\n",
        ")\n",
        "\n",
        "# Candidate provider x shift (small demo sizes make crossJoin acceptable)\n",
        "cand = shift_req.select(\n",
        "    \"shift_id\", \"facility_id\", \"required_procedure_code\", \"requires_privilege\", \"requires_acls\"\n",
        ").crossJoin(base_ok)\n",
        "\n",
        "# Privilege requirement\n",
        "priv_ok = (\n",
        "    current_priv\n",
        "      .filter(F.col(\"privilege_status\") == \"ACTIVE\")\n",
        "      .select(\n",
        "          \"provider_id\",\n",
        "          \"facility_id\",\n",
        "          F.col(\"procedure_code\").alias(\"required_procedure_code\")\n",
        "      )\n",
        "      .withColumn(\"has_priv\", F.lit(1))\n",
        ")\n",
        "cand = cand.join(priv_ok, [\"provider_id\", \"facility_id\", \"required_procedure_code\"], \"left\")\n",
        "\n",
        "# ACLS requirement\n",
        "cand = cand.join(acls_ok.withColumn(\"has_acls\", F.lit(1)), \"provider_id\", \"left\")\n",
        "\n",
        "eligible = (\n",
        "    cand\n",
        "      .withColumn(\n",
        "          \"eligible\",\n",
        "          F.when((F.col(\"requires_privilege\") == True) & (F.col(\"has_priv\").isNull()), F.lit(False))\n",
        "           .when((F.col(\"requires_acls\") == True) & (F.col(\"has_acls\").isNull()), F.lit(False))\n",
        "           .otherwise(F.lit(True))\n",
        "      )\n",
        "      .filter(F.col(\"eligible\") == True)\n",
        "      .select(\"shift_id\", \"provider_id\")\n",
        ")\n",
        "\n",
        "assigned = (\n",
        "    assign\n",
        "      .filter(F.col(\"assignment_status\") == \"ASSIGNED\")\n",
        "      .groupBy(\"shift_id\")\n",
        "      .agg(F.countDistinct(\"provider_id\").alias(\"assigned_count\"))\n",
        ")\n",
        "eligible_cnt = eligible.groupBy(\"shift_id\").agg(F.countDistinct(\"provider_id\").alias(\"eligible_provider_count\"))\n",
        "\n",
        "staffing_gaps = (\n",
        "    shift_req\n",
        "      .join(assigned, \"shift_id\", \"left\")\n",
        "      .join(eligible_cnt, \"shift_id\", \"left\")\n",
        "      .fillna({\"assigned_count\": 0, \"eligible_provider_count\": 0})\n",
        "      .withColumn(\"gap_count\", F.greatest(F.col(\"required_count\") - F.col(\"assigned_count\"), F.lit(0)))\n",
        "      .withColumn(\n",
        "          \"risk_reason\",\n",
        "          F.when(F.col(\"gap_count\") <= 0, F.lit(\"OK\"))\n",
        "           .when(F.col(\"eligible_provider_count\") == 0, F.lit(\"No eligible providers\"))\n",
        "           .when(F.col(\"assigned_count\") == 0, F.lit(\"Unfilled shift\"))\n",
        "           .otherwise(F.lit(\"Partial coverage\"))\n",
        "      )\n",
        "      .withColumn(\n",
        "          \"risk_level\",\n",
        "          F.when((F.col(\"gap_count\") > 0) & (F.col(\"eligible_provider_count\") == 0), F.lit(\"HIGH\"))\n",
        "           .when(F.col(\"gap_count\") >= 2, F.lit(\"HIGH\"))\n",
        "           .when(F.col(\"gap_count\") == 1, F.lit(\"MEDIUM\"))\n",
        "           .otherwise(F.lit(\"LOW\"))\n",
        "      )\n",
        "      .withColumn(\"last_built_at\", F.current_timestamp())\n",
        "      .select(\n",
        "          \"shift_id\", \"facility_id\", \"facility_name\", \"start_ts\", \"end_ts\",\n",
        "          \"required_procedure_code\", \"procedure_name\",\n",
        "          \"required_count\", \"assigned_count\", \"eligible_provider_count\",\n",
        "          \"gap_count\", \"risk_reason\", \"risk_level\", \"last_built_at\"\n",
        "      )\n",
        ")\n",
        "\n",
        "staffing_gaps.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"staffing_gaps\")\n",
        ")\n",
        "\n",
        "# Recommendations: up to 5 eligible providers per shift\n",
        "rank_w = Window.partitionBy(\"shift_id\").orderBy(F.rand(SEED))\n",
        "shift_recommendations = (\n",
        "    eligible\n",
        "      .withColumn(\"rn\", F.row_number().over(rank_w))\n",
        "      .filter(F.col(\"rn\") <= 5)\n",
        "      .groupBy(\"shift_id\")\n",
        "      .agg(F.collect_list(\"provider_id\").alias(\"recommended_provider_ids\"))\n",
        ")\n",
        "\n",
        "shift_recommendations.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"shift_recommendations\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gold: credential_risk + kpi_summary_daily\n",
        "Buckets based on days left and a simple daily KPI snapshot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "credential_risk = (\n",
        "    current_cred\n",
        "      .withColumn(\n",
        "          \"risk_bucket\",\n",
        "          F.when(F.col(\"days_until_expiration\") < 0, F.lit(\"EXPIRED\"))\n",
        "           .when(F.col(\"days_until_expiration\") <= 14, F.lit(\"0-14\"))\n",
        "           .when(F.col(\"days_until_expiration\") <= 30, F.lit(\"15-30\"))\n",
        "           .when(F.col(\"days_until_expiration\") <= 90, F.lit(\"31-90\"))\n",
        "           .otherwise(F.lit(\">90\"))\n",
        "      )\n",
        "      .withColumn(\"last_built_at\", F.current_timestamp())\n",
        ")\n",
        "\n",
        "credential_risk.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"credential_risk\")\n",
        ")\n",
        "\n",
        "# Daily KPIs\n",
        "providers_total = provider.count()\n",
        "providers_pending = provider.join(base_ok, \"provider_id\", \"left_anti\").count()\n",
        "providers_expiring_30d = (\n",
        "    current_cred\n",
        "      .filter(F.col(\"cred_type\") == \"STATE_MED_LICENSE\")\n",
        "      .filter((F.col(\"days_until_expiration\") >= 0) & (F.col(\"days_until_expiration\") <= 30))\n",
        "      .select(\"provider_id\").distinct().count()\n",
        ")\n",
        "\n",
        "daily_revenue_at_risk_est = float(providers_expiring_30d) * 7500.0  # demo estimate\n",
        "\n",
        "kpi_df = spark.createDataFrame(\n",
        "    [(\n",
        "        datetime.utcnow().date(),\n",
        "        int(providers_total),\n",
        "        int(providers_pending),\n",
        "        int(providers_expiring_30d),\n",
        "        float(daily_revenue_at_risk_est),\n",
        "        datetime.utcnow()\n",
        "    )],\n",
        "    [\"kpi_date\", \"providers_total\", \"providers_pending\", \"providers_expiring_30d\", \"daily_revenue_at_risk_est\", \"last_built_at\"]\n",
        ")\n",
        "\n",
        "kpi_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"kpi_summary_daily\")\n",
        ")\n",
        "\n",
        "display(spark.read.table(fq(schema_gold, \"staffing_gaps\")).orderBy(F.desc(\"gap_count\")).limit(25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be19dc5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Gold: risk_actions (closed-loop mitigation workflow)\n",
        "# This table is intentionally simple: it captures operational actions tied to either a SHIFT or a PROVIDER.\n",
        "\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "base_ts = datetime(2026, 1, 1, 8, 0, 0)\n",
        "\n",
        "# Stable UUIDs make this table deterministic/reproducible across reruns and across accounts.\n",
        "def _action_id(key: str) -> str:\n",
        "    return str(uuid.uuid5(uuid.NAMESPACE_URL, f\"risk-action-{SEED}-{key}\"))\n",
        "\n",
        "_action_id_udf = F.udf(_action_id)\n",
        "\n",
        "owners = [\"staffing_coordinator\", \"med_staff_office\", \"ops_manager\"]\n",
        "\n",
        "# SHIFT actions (from staffing gaps)\n",
        "gaps_for_actions = (\n",
        "    spark.read.table(fq(schema_gold, \"staffing_gaps\"))\n",
        "      .filter(F.col(\"gap_count\") > 0)\n",
        "      .filter(F.col(\"risk_level\").isin([\"HIGH\", \"MEDIUM\"]))\n",
        "      .orderBy(F.desc(\"gap_count\"), F.asc(\"start_ts\"))\n",
        "      .limit(75)\n",
        ")\n",
        "\n",
        "shift_actions = (\n",
        "    gaps_for_actions\n",
        "      .withColumn(\"action_id\", _action_id_udf(F.concat(F.lit(\"SHIFT:\"), F.col(\"shift_id\"))))\n",
        "      .withColumn(\"entity_type\", F.lit(\"SHIFT\"))\n",
        "      .withColumn(\"entity_id\", F.col(\"shift_id\"))\n",
        "      .withColumn(\"action_type\", F.lit(\"OUTREACH\"))\n",
        "      .withColumn(\"status\", F.lit(\"OPEN\"))\n",
        "      .withColumn(\n",
        "          \"priority\",\n",
        "          F.when(F.col(\"risk_level\") == \"HIGH\", F.lit(\"HIGH\")).otherwise(F.lit(\"MEDIUM\"))\n",
        "      )\n",
        "      .withColumn(\"owner\", F.element_at(F.array([F.lit(o) for o in owners]), (F.pmod(F.hash(F.col(\"shift_id\")), F.lit(len(owners))) + 1)))\n",
        "      .withColumn(\"notes\", F.concat(F.lit(\"Outreach for uncovered shift (gap=\"), F.col(\"gap_count\").cast(\"string\"), F.lit(\")\")))\n",
        "      .withColumn(\"created_at\", F.lit(base_ts) + F.expr(\"INTERVAL 1 HOURS\") * F.pmod(F.hash(F.col(\"shift_id\")), F.lit(120)))\n",
        "      .withColumn(\"updated_at\", F.col(\"created_at\"))\n",
        "      .withColumn(\"resolved_at\", F.lit(None).cast(\"timestamp\"))\n",
        "      .withColumn(\"last_built_at\", F.current_timestamp())\n",
        "      .select(\n",
        "          \"action_id\",\n",
        "          \"entity_type\",\n",
        "          \"entity_id\",\n",
        "          \"facility_id\",\n",
        "          \"action_type\",\n",
        "          \"status\",\n",
        "          \"priority\",\n",
        "          \"owner\",\n",
        "          \"created_at\",\n",
        "          \"updated_at\",\n",
        "          \"resolved_at\",\n",
        "          \"notes\",\n",
        "          \"last_built_at\",\n",
        "      )\n",
        ")\n",
        "\n",
        "# PROVIDER actions (from expiring credentials)\n",
        "provider_home = spark.read.table(fq(schema_gold, \"provider_360_flat\")).select(\"provider_id\", \"home_facility_id\")\n",
        "\n",
        "cred_for_actions = (\n",
        "    spark.read.table(fq(schema_gold, \"credential_risk\"))\n",
        "      .filter(F.col(\"cred_type\").isin([\"STATE_MED_LICENSE\", \"ACLS\"]))\n",
        "      .filter(F.col(\"risk_bucket\").isin([\"EXPIRED\", \"0-14\", \"15-30\"]))\n",
        "      .orderBy(F.asc(\"days_until_expiration\"))\n",
        "      .limit(75)\n",
        "      .join(provider_home, \"provider_id\", \"left\")\n",
        ")\n",
        "\n",
        "provider_actions = (\n",
        "    cred_for_actions\n",
        "      .withColumn(\"action_id\", _action_id_udf(F.concat(F.lit(\"PROVIDER:\"), F.col(\"provider_id\"), F.lit(\":\"), F.col(\"cred_type\"))))\n",
        "      .withColumn(\"entity_type\", F.lit(\"PROVIDER\"))\n",
        "      .withColumn(\"entity_id\", F.col(\"provider_id\"))\n",
        "      .withColumn(\"facility_id\", F.col(\"home_facility_id\"))\n",
        "      .withColumn(\"action_type\", F.lit(\"CREDENTIAL_EXPEDITE\"))\n",
        "      .withColumn(\"status\", F.lit(\"OPEN\"))\n",
        "      .withColumn(\n",
        "          \"priority\",\n",
        "          F.when(F.col(\"risk_bucket\") == \"EXPIRED\", F.lit(\"HIGH\"))\n",
        "           .when(F.col(\"risk_bucket\") == \"0-14\", F.lit(\"HIGH\"))\n",
        "           .otherwise(F.lit(\"MEDIUM\"))\n",
        "      )\n",
        "      .withColumn(\"owner\", F.lit(\"med_staff_office\"))\n",
        "      .withColumn(\n",
        "          \"notes\",\n",
        "          F.concat(\n",
        "              F.lit(\"Credential renewal outreach: \"),\n",
        "              F.col(\"cred_type\"),\n",
        "              F.lit(\" (days_left=\"),\n",
        "              F.col(\"days_until_expiration\").cast(\"string\"),\n",
        "              F.lit(\")\")\n",
        "          )\n",
        "      )\n",
        "      .withColumn(\"created_at\", F.lit(base_ts) + F.expr(\"INTERVAL 1 HOURS\") * F.pmod(F.hash(F.col(\"provider_id\")), F.lit(120)))\n",
        "      .withColumn(\"updated_at\", F.col(\"created_at\"))\n",
        "      .withColumn(\"resolved_at\", F.lit(None).cast(\"timestamp\"))\n",
        "      .withColumn(\"last_built_at\", F.current_timestamp())\n",
        "      .select(\n",
        "          \"action_id\",\n",
        "          \"entity_type\",\n",
        "          \"entity_id\",\n",
        "          \"facility_id\",\n",
        "          \"action_type\",\n",
        "          \"status\",\n",
        "          \"priority\",\n",
        "          \"owner\",\n",
        "          \"created_at\",\n",
        "          \"updated_at\",\n",
        "          \"resolved_at\",\n",
        "          \"notes\",\n",
        "          \"last_built_at\",\n",
        "      )\n",
        ")\n",
        "\n",
        "risk_actions = shift_actions.unionByName(provider_actions)\n",
        "\n",
        "risk_actions.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"risk_actions\")\n",
        ")\n",
        "\n",
        "display(risk_actions.orderBy(F.desc(\"priority\"), F.desc(\"created_at\")).limit(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4edf53d8",
      "metadata": {},
      "source": [
        "## Gold: nurse_staffing_summary\n",
        "Daily staffing summary per unit with nurse-to-patient ratios, staffing status, and labor costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9e3512",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load reference tables for nurse staffing\n",
        "ref_unit = spark.read.table(fq(schema_ref, \"unit\"))\n",
        "nurse_assign_current = spark.read.table(fq(schema_silver, \"nurse_assignment_current\"))\n",
        "\n",
        "# Join nurse assignments with provider info to get employment type and hourly rate\n",
        "nurse_with_info = (\n",
        "    nurse_assign_current\n",
        "      .join(\n",
        "          provider.select(\"provider_id\", \"employment_type\", \"hourly_rate\"),\n",
        "          \"provider_id\",\n",
        "          \"left\"\n",
        "      )\n",
        ")\n",
        "\n",
        "# Aggregate by unit\n",
        "staffing_by_unit = (\n",
        "    nurse_with_info\n",
        "      .groupBy(\"unit_id\")\n",
        "      .agg(\n",
        "          F.count(\"*\").alias(\"nurses_assigned\"),\n",
        "          F.sum(F.when(F.col(\"employment_type\") == \"INTERNAL\", 1).otherwise(0)).alias(\"nurses_internal\"),\n",
        "          F.sum(F.when(F.col(\"employment_type\") == \"CONTRACT\", 1).otherwise(0)).alias(\"nurses_contract\"),\n",
        "          F.sum(F.when(F.col(\"employment_type\") == \"AGENCY\", 1).otherwise(0)).alias(\"nurses_agency\"),\n",
        "          F.sum(F.col(\"hourly_rate\") * 12).alias(\"labor_cost_daily\"),  # 12-hour shifts\n",
        "      )\n",
        ")\n",
        "\n",
        "# Simulate census data (in production this would come from ADT/census table)\n",
        "# For demo: census = bed_count * random factor (0.6-0.9)\n",
        "import random\n",
        "random.seed(SEED)\n",
        "\n",
        "census_data = []\n",
        "for row in ref_unit.collect():\n",
        "    census = int(row[\"bed_count\"] * random.uniform(0.6, 0.9))\n",
        "    census_data.append((row[\"unit_id\"], census))\n",
        "\n",
        "census_df = spark.createDataFrame(census_data, [\"unit_id\", \"current_census\"])\n",
        "\n",
        "# Build nurse staffing summary\n",
        "nurse_staffing_summary = (\n",
        "    ref_unit\n",
        "      .join(ref_facility.select(\"facility_id\", \"facility_name\"), \"facility_id\", \"left\")\n",
        "      .join(census_df, \"unit_id\", \"left\")\n",
        "      .join(staffing_by_unit, \"unit_id\", \"left\")\n",
        "      .fillna({\n",
        "          \"nurses_assigned\": 0,\n",
        "          \"nurses_internal\": 0,\n",
        "          \"nurses_contract\": 0,\n",
        "          \"nurses_agency\": 0,\n",
        "          \"labor_cost_daily\": 0.0,\n",
        "          \"current_census\": 0,\n",
        "      })\n",
        "      .withColumn(\"summary_date\", F.current_date())\n",
        "      .withColumn(\"nurses_required\", F.ceil(F.col(\"current_census\") / F.col(\"target_ratio\")).cast(\"int\"))\n",
        "      .withColumn(\"staffing_delta\", F.col(\"nurses_assigned\") - F.col(\"nurses_required\"))\n",
        "      .withColumn(\n",
        "          \"staffing_status\",\n",
        "          F.when(F.col(\"nurses_assigned\") < F.col(\"nurses_required\"), F.lit(\"UNDERSTAFFED\"))\n",
        "           .when(F.col(\"nurses_assigned\") > F.col(\"nurses_required\") + 1, F.lit(\"OVERSTAFFED\"))\n",
        "           .otherwise(F.lit(\"OPTIMAL\"))\n",
        "      )\n",
        "      .withColumn(\"last_built_at\", F.current_timestamp())\n",
        "      .select(\n",
        "          \"summary_date\",\n",
        "          \"unit_id\",\n",
        "          \"facility_id\",\n",
        "          \"facility_name\",\n",
        "          \"unit_name\",\n",
        "          \"unit_type\",\n",
        "          \"bed_count\",\n",
        "          \"current_census\",\n",
        "          \"target_ratio\",\n",
        "          \"nurses_required\",\n",
        "          \"nurses_assigned\",\n",
        "          \"nurses_internal\",\n",
        "          \"nurses_contract\",\n",
        "          \"nurses_agency\",\n",
        "          \"staffing_delta\",\n",
        "          \"staffing_status\",\n",
        "          \"labor_cost_daily\",\n",
        "          \"last_built_at\",\n",
        "      )\n",
        ")\n",
        "\n",
        "nurse_staffing_summary.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"nurse_staffing_summary\")\n",
        ")\n",
        "\n",
        "display(nurse_staffing_summary.orderBy(F.desc(\"staffing_status\"), \"unit_name\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b00d08",
      "metadata": {},
      "source": [
        "## Gold: credential_gaps (nurse staffing)\n",
        "Units where required certifications are missing among assigned nurses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a6377d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load unit certification requirements\n",
        "unit_cert = spark.read.table(fq(schema_ref, \"unit_certification\")).filter(F.col(\"is_required\") == True)\n",
        "\n",
        "# Get nurses assigned to each unit today\n",
        "nurses_by_unit = (\n",
        "    nurse_assign_current\n",
        "      .groupBy(\"unit_id\")\n",
        "      .agg(\n",
        "          F.count(\"*\").alias(\"nurses_assigned\"),\n",
        "          F.collect_list(\"provider_id\").alias(\"nurse_ids\"),\n",
        "      )\n",
        ")\n",
        "\n",
        "# For each unit + required cert, count how many nurses have that cert\n",
        "# Join unit -> unit_type -> unit_cert requirements\n",
        "unit_with_type = ref_unit.select(\"unit_id\", \"unit_type\", \"unit_name\", \"facility_id\")\n",
        "\n",
        "unit_cert_requirements = (\n",
        "    unit_with_type\n",
        "      .join(unit_cert, \"unit_type\", \"inner\")\n",
        "      .join(ref_facility.select(\"facility_id\", \"facility_name\"), \"facility_id\", \"left\")\n",
        ")\n",
        "\n",
        "# For each nurse, check which certs they have (using credential_risk table)\n",
        "nurse_certs = (\n",
        "    spark.read.table(fq(schema_gold, \"credential_risk\"))\n",
        "      .filter(F.col(\"cred_status\") == \"ACTIVE\")\n",
        "      .filter(F.col(\"days_until_expiration\") >= 0)\n",
        "      .select(\"provider_id\", \"cred_type\")\n",
        "      .distinct()\n",
        ")\n",
        "\n",
        "# Cross join nurses with their assigned units and check cert coverage\n",
        "nurse_unit_cert_check = (\n",
        "    nurse_assign_current\n",
        "      .select(\"unit_id\", \"provider_id\")\n",
        "      .join(unit_with_type, \"unit_id\", \"left\")\n",
        "      .join(unit_cert, \"unit_type\", \"inner\")\n",
        "      .join(nurse_certs, [\"provider_id\", \"cred_type\"], \"left_outer\")\n",
        "      .withColumn(\"has_cert\", F.when(nurse_certs[\"cred_type\"].isNotNull(), F.lit(1)).otherwise(F.lit(0)))\n",
        ")\n",
        "\n",
        "# Aggregate: for each unit + cert type, count nurses with/without cert\n",
        "credential_gaps_raw = (\n",
        "    nurse_unit_cert_check\n",
        "      .groupBy(\"unit_id\", \"unit_type\", \"cred_type\")\n",
        "      .agg(\n",
        "          F.count(\"*\").alias(\"nurses_assigned\"),\n",
        "          F.sum(\"has_cert\").alias(\"nurses_with_cert\"),\n",
        "          F.collect_list(\n",
        "              F.when(F.col(\"has_cert\") == 0, F.col(\"provider_id\"))\n",
        "          ).alias(\"affected_nurse_ids_raw\"),\n",
        "      )\n",
        "      .withColumn(\"nurses_missing_cert\", F.col(\"nurses_assigned\") - F.col(\"nurses_with_cert\"))\n",
        "      .filter(F.col(\"nurses_missing_cert\") > 0)  # Only show gaps\n",
        ")\n",
        "\n",
        "# Calculate severity\n",
        "credential_gaps = (\n",
        "    credential_gaps_raw\n",
        "      .join(unit_with_type.select(\"unit_id\", \"unit_name\", \"facility_id\"), \"unit_id\", \"left\")\n",
        "      .join(ref_facility.select(\"facility_id\", \"facility_name\"), \"facility_id\", \"left\")\n",
        "      .withColumn(\n",
        "          \"gap_severity\",\n",
        "          F.when(F.col(\"nurses_missing_cert\") >= F.col(\"nurses_assigned\") * 0.5, F.lit(\"CRITICAL\"))\n",
        "           .when(F.col(\"nurses_missing_cert\") >= F.col(\"nurses_assigned\") * 0.25, F.lit(\"HIGH\"))\n",
        "           .when(F.col(\"nurses_missing_cert\") > 0, F.lit(\"MEDIUM\"))\n",
        "           .otherwise(F.lit(\"LOW\"))\n",
        "      )\n",
        "      .withColumn(\n",
        "          \"affected_nurse_ids\",\n",
        "          F.expr(\"filter(affected_nurse_ids_raw, x -> x IS NOT NULL)\")\n",
        "      )\n",
        "      .withColumn(\"required_cred_type\", F.col(\"cred_type\"))\n",
        "      .select(\n",
        "          \"unit_id\",\n",
        "          \"facility_id\",\n",
        "          \"facility_name\",\n",
        "          \"unit_name\",\n",
        "          \"unit_type\",\n",
        "          \"required_cred_type\",\n",
        "          \"nurses_assigned\",\n",
        "          \"nurses_with_cert\",\n",
        "          \"nurses_missing_cert\",\n",
        "          \"gap_severity\",\n",
        "          \"affected_nurse_ids\",\n",
        "      )\n",
        ")\n",
        "\n",
        "credential_gaps.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
        "    fq(schema_gold, \"credential_gaps\")\n",
        ")\n",
        "\n",
        "display(credential_gaps.orderBy(F.desc(\"gap_severity\"), \"unit_name\"))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
