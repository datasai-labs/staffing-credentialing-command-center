{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6546c0bf",
      "metadata": {},
      "source": [
        "# 02 â€” Generate providers + credentials (bronze)\n",
        "\n",
        "Writes synthetic provider master data, append-only credential events, privileges, and payer enrollment into `bronze` tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install faker==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (Databricks widgets)\n",
        "# These widgets make the demo portable across workspaces/accounts.\n",
        "# If you're running this outside a Databricks notebook, it will fall back to defaults.\n",
        "\n",
        "DEFAULT_CATALOG = \"staffing_catalog\"\n",
        "DEFAULT_SCHEMA_REF = \"credentialing_ref\"\n",
        "DEFAULT_SCHEMA_BRONZE = \"credentialing_bronze\"\n",
        "DEFAULT_SCHEMA_SILVER = \"credentialing_silver\"\n",
        "DEFAULT_SCHEMA_GOLD = \"credentialing_gold\"\n",
        "\n",
        "DEFAULT_N_PROVIDERS = 200\n",
        "DEFAULT_DAYS_SCHEDULE = 14\n",
        "DEFAULT_SEED = 42\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"catalog\", DEFAULT_CATALOG, \"Catalog\")\n",
        "    dbutils.widgets.text(\"schema_ref\", DEFAULT_SCHEMA_REF, \"Schema (ref)\")\n",
        "    dbutils.widgets.text(\"schema_bronze\", DEFAULT_SCHEMA_BRONZE, \"Schema (bronze)\")\n",
        "    dbutils.widgets.text(\"schema_silver\", DEFAULT_SCHEMA_SILVER, \"Schema (silver)\")\n",
        "    dbutils.widgets.text(\"schema_gold\", DEFAULT_SCHEMA_GOLD, \"Schema (gold)\")\n",
        "\n",
        "    dbutils.widgets.text(\"n_providers\", str(DEFAULT_N_PROVIDERS), \"N providers\")\n",
        "    dbutils.widgets.text(\"days_schedule\", str(DEFAULT_DAYS_SCHEDULE), \"Days schedule\")\n",
        "    dbutils.widgets.text(\"seed\", str(DEFAULT_SEED), \"Random seed\")\n",
        "\n",
        "    catalog = dbutils.widgets.get(\"catalog\") or DEFAULT_CATALOG\n",
        "    schema_ref = dbutils.widgets.get(\"schema_ref\") or DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = dbutils.widgets.get(\"schema_bronze\") or DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = dbutils.widgets.get(\"schema_silver\") or DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = dbutils.widgets.get(\"schema_gold\") or DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = int(dbutils.widgets.get(\"n_providers\") or DEFAULT_N_PROVIDERS)\n",
        "    DAYS_SCHEDULE = int(dbutils.widgets.get(\"days_schedule\") or DEFAULT_DAYS_SCHEDULE)\n",
        "    SEED = int(dbutils.widgets.get(\"seed\") or DEFAULT_SEED)\n",
        "except Exception:\n",
        "    catalog = DEFAULT_CATALOG\n",
        "    schema_ref = DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = DEFAULT_N_PROVIDERS\n",
        "    DAYS_SCHEDULE = DEFAULT_DAYS_SCHEDULE\n",
        "    SEED = DEFAULT_SEED\n",
        "\n",
        "# Derived helpers\n",
        "fq = lambda sch, tbl: f\"{catalog}.{sch}.{tbl}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog bootstrap (you may need permissions to create catalogs/schemas)\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "spark.sql(f\"USE CATALOG {catalog}\")\n",
        "for sch in [schema_ref, schema_bronze, schema_silver, schema_gold]:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{sch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load reference data\n",
        "Requires that Notebook 01 has run to create `ref.*` tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "from faker import Faker\n",
        "\n",
        "random.seed(SEED)\n",
        "fake = Faker()\n",
        "fake.seed_instance(SEED)\n",
        "\n",
        "facility_ids = [r[\"facility_id\"] for r in spark.read.table(fq(schema_ref, \"facility\")).select(\"facility_id\").collect()]\n",
        "cred_types = [r[\"cred_type\"] for r in spark.read.table(fq(schema_ref, \"credential_type\")).select(\"cred_type\").collect()]\n",
        "payer_ids = [r[\"payer_id\"] for r in spark.read.table(fq(schema_ref, \"payer\")).select(\"payer_id\").collect()]\n",
        "procedures_df = spark.read.table(fq(schema_ref, \"procedure\")).select(\"procedure_code\", \"requires_privilege\")\n",
        "\n",
        "SPECIALTIES = [\"Emergency Medicine\", \"Surgery\", \"Anesthesiology\", \"Critical Care\", \"Cardiology\"]\n",
        "PROVIDER_STATUSES = [\"ACTIVE\", \"ACTIVE\", \"ACTIVE\", \"INACTIVE\", \"ON_LEAVE\"]\n",
        "\n",
        "# Fixed base timestamp keeps demo reproducible (even across reruns)\n",
        "base_ts = datetime(2026, 1, 1, 8, 0, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2227ddd8",
      "metadata": {},
      "source": [
        "## bronze.provider_raw\n",
        "Columns: (id, name, specialty, home_facility_id, hired_at, provider_status, created_at, employment_type, hourly_rate, primary_unit_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load unit IDs for nurse staffing assignment\n",
        "unit_ids = [r[\"unit_id\"] for r in spark.read.table(fq(schema_ref, \"unit\")).select(\"unit_id\").collect()]\n",
        "\n",
        "# Employment types and hourly rates for nurse staffing cost analysis\n",
        "EMPLOYMENT_TYPES = [\"INTERNAL\", \"INTERNAL\", \"INTERNAL\", \"CONTRACT\", \"AGENCY\"]  # Weighted toward internal\n",
        "HOURLY_RATES = {\"INTERNAL\": 50.0, \"CONTRACT\": 75.0, \"AGENCY\": 95.0}\n",
        "\n",
        "provider_rows = []\n",
        "for i in range(N_PROVIDERS):\n",
        "    provider_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"provider-{SEED}-{i}\"))\n",
        "    provider_name = fake.name()\n",
        "    specialty = random.choice(SPECIALTIES)\n",
        "    home_facility_id = random.choice(facility_ids)\n",
        "    hired_at = base_ts.date() - timedelta(days=random.randint(30, 3650))\n",
        "    provider_status = random.choice(PROVIDER_STATUSES)\n",
        "    created_at = base_ts + timedelta(minutes=i)\n",
        "    # Nurse staffing fields\n",
        "    employment_type = random.choice(EMPLOYMENT_TYPES)\n",
        "    hourly_rate = HOURLY_RATES[employment_type] + random.uniform(-5, 10)  # Add some variance\n",
        "    primary_unit_id = random.choice(unit_ids) if random.random() < 0.7 else None  # 70% have a primary unit\n",
        "    provider_rows.append((provider_id, provider_name, specialty, home_facility_id, hired_at, provider_status, created_at, employment_type, hourly_rate, primary_unit_id))\n",
        "\n",
        "provider_schema = StructType([\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"provider_name\", StringType(), False),\n",
        "    StructField(\"specialty\", StringType(), False),\n",
        "    StructField(\"home_facility_id\", StringType(), False),\n",
        "    StructField(\"hired_at\", DateType(), False),\n",
        "    StructField(\"provider_status\", StringType(), False),\n",
        "    StructField(\"created_at\", TimestampType(), False),\n",
        "    StructField(\"employment_type\", StringType(), False),  # INTERNAL, CONTRACT, AGENCY\n",
        "    StructField(\"hourly_rate\", FloatType(), False),       # For labor cost calculations\n",
        "    StructField(\"primary_unit_id\", StringType(), True),   # Home unit assignment (nullable)\n",
        "])\n",
        "\n",
        "provider_df = spark.createDataFrame(provider_rows, provider_schema)\n",
        "provider_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"provider_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.credential_event_raw\n",
        "Append-only style events: (event_id, provider_id, cred_type, issued_at, expires_at, verified_at, source_system, cred_status, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "event_rows = []\n",
        "event_i = 0\n",
        "\n",
        "for p in provider_rows:\n",
        "    provider_id = p[0]\n",
        "    hired_at = p[4]\n",
        "    for ct in cred_types:\n",
        "        n_events = 1 + (random.randint(0, 2) if ct in [\"STATE_MED_LICENSE\", \"ACLS\"] else random.randint(0, 1))\n",
        "        issued0 = datetime.combine(hired_at, datetime.min.time()) + timedelta(days=random.randint(0, 60))\n",
        "        for k in range(n_events):\n",
        "            event_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"cred-event-{SEED}-{event_i}\"))\n",
        "            issued_at = issued0 + timedelta(days=365 * k + random.randint(0, 30))\n",
        "            cycle_days = 730 if ct in [\"STATE_MED_LICENSE\", \"ACLS\"] else 1095\n",
        "            drift = random.randint(-120, 240)  # creates some expired credentials for risk analytics\n",
        "            expires_at = issued_at + timedelta(days=cycle_days + drift)\n",
        "            verified_at = issued_at + timedelta(days=random.randint(1, 14)) if random.random() < 0.85 else None\n",
        "            source_system = random.choice([\"CRED_SYS_A\", \"CRED_SYS_B\"])\n",
        "            cred_status = \"EXPIRED\" if expires_at.date() < base_ts.date() else random.choice([\"ACTIVE\", \"ACTIVE\", \"PENDING_REVIEW\"])\n",
        "            ingested_at = base_ts + timedelta(seconds=event_i)\n",
        "            event_rows.append((event_id, provider_id, ct, issued_at, expires_at, verified_at, source_system, cred_status, ingested_at))\n",
        "            event_i += 1\n",
        "\n",
        "cred_schema = StructType([\n",
        "    StructField(\"event_id\", StringType(), False),\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"cred_type\", StringType(), False),\n",
        "    StructField(\"issued_at\", TimestampType(), False),\n",
        "    StructField(\"expires_at\", TimestampType(), False),\n",
        "    StructField(\"verified_at\", TimestampType(), True),\n",
        "    StructField(\"source_system\", StringType(), False),\n",
        "    StructField(\"cred_status\", StringType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "cred_event_df = spark.createDataFrame(event_rows, cred_schema)\n",
        "cred_event_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"credential_event_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.privilege_raw\n",
        "Privileges by provider/facility/procedure: (provider_id, facility_id, procedure_code, granted_at, privilege_status, source_system, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proc_priv = [r[\"procedure_code\"] for r in procedures_df.filter(F.col(\"requires_privilege\") == True).select(\"procedure_code\").collect()]\n",
        "\n",
        "priv_rows = []\n",
        "priv_i = 0\n",
        "\n",
        "for p in provider_rows:\n",
        "    provider_id = p[0]\n",
        "    provider_status = p[5]\n",
        "    max_priv = 4 if provider_status == \"ACTIVE\" else 2\n",
        "    n_priv = random.randint(0, max_priv)\n",
        "    for proc in random.sample(proc_priv, k=min(n_priv, len(proc_priv))):\n",
        "        facility_id = random.choice(facility_ids)\n",
        "        granted_at = base_ts - timedelta(days=random.randint(0, 1200))\n",
        "        privilege_status = random.choice([\"ACTIVE\", \"ACTIVE\", \"SUSPENDED\", \"REVOKED\"])\n",
        "        source_system = random.choice([\"PRIV_SYS_A\", \"PRIV_SYS_B\"])\n",
        "        ingested_at = base_ts + timedelta(seconds=100000 + priv_i)\n",
        "        priv_rows.append((provider_id, facility_id, proc, granted_at, privilege_status, source_system, ingested_at))\n",
        "        priv_i += 1\n",
        "\n",
        "priv_schema = StructType([\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"facility_id\", StringType(), False),\n",
        "    StructField(\"procedure_code\", StringType(), False),\n",
        "    StructField(\"granted_at\", TimestampType(), False),\n",
        "    StructField(\"privilege_status\", StringType(), False),\n",
        "    StructField(\"source_system\", StringType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "priv_df = spark.createDataFrame(priv_rows, priv_schema)\n",
        "priv_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"privilege_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.payer_enrollment_raw\n",
        "Enrollment rows: (provider_id, payer_id, enrollment_status, effective_at, source_system, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enr_rows = []\n",
        "enr_i = 0\n",
        "\n",
        "for p in provider_rows:\n",
        "    provider_id = p[0]\n",
        "    provider_status = p[5]\n",
        "    n = random.randint(1, 3) if provider_status == \"ACTIVE\" else random.randint(0, 2)\n",
        "    chosen = random.sample(payer_ids, k=min(max(n, 0), len(payer_ids)))\n",
        "    for payer_id in chosen:\n",
        "        effective_at = base_ts.date() - timedelta(days=random.randint(0, 900))\n",
        "        if provider_status != \"ACTIVE\" and random.random() < 0.6:\n",
        "            enrollment_status = \"INACTIVE\"\n",
        "        else:\n",
        "            enrollment_status = random.choice([\"ACTIVE\", \"ACTIVE\", \"PENDING\"])\n",
        "        source_system = random.choice([\"PAYER_SYS_A\", \"PAYER_SYS_B\"])\n",
        "        ingested_at = base_ts + timedelta(seconds=200000 + enr_i)\n",
        "        enr_rows.append((provider_id, payer_id, enrollment_status, effective_at, source_system, ingested_at))\n",
        "        enr_i += 1\n",
        "\n",
        "enr_schema = StructType([\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"payer_id\", StringType(), False),\n",
        "    StructField(\"enrollment_status\", StringType(), False),\n",
        "    StructField(\"effective_at\", DateType(), False),\n",
        "    StructField(\"source_system\", StringType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "enr_df = spark.createDataFrame(enr_rows, enr_schema)\n",
        "enr_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"payer_enrollment_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate\n",
        "Counts + simple groupBy (providers by specialty).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Counts:\")\n",
        "for t in [\"provider_raw\", \"credential_event_raw\", \"privilege_raw\", \"payer_enrollment_raw\"]:\n",
        "    print(f\"{fq(schema_bronze, t)}: {spark.read.table(fq(schema_bronze, t)).count():,}\")\n",
        "\n",
        "display(\n",
        "    spark.read.table(fq(schema_bronze, \"provider_raw\"))\n",
        "        .groupBy(\"specialty\")\n",
        "        .count()\n",
        "        .orderBy(F.desc(\"count\"))\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
