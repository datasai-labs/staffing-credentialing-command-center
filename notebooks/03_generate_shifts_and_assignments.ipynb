{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 â€” Generate shifts + assignments (bronze)\n",
        "\n",
        "Creates upcoming shifts for the next `DAYS_SCHEDULE` days and assignments with intentional gaps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install faker==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (Databricks widgets)\n",
        "# These widgets make the demo portable across workspaces/accounts.\n",
        "# If you're running this outside a Databricks notebook, it will fall back to defaults.\n",
        "\n",
        "DEFAULT_CATALOG = \"rtpa_catalog\"\n",
        "DEFAULT_SCHEMA_REF = \"credentialing_ref\"\n",
        "DEFAULT_SCHEMA_BRONZE = \"credentialing_bronze\"\n",
        "DEFAULT_SCHEMA_SILVER = \"credentialing_silver\"\n",
        "DEFAULT_SCHEMA_GOLD = \"credentialing_gold\"\n",
        "\n",
        "DEFAULT_N_PROVIDERS = 200\n",
        "DEFAULT_DAYS_SCHEDULE = 14\n",
        "DEFAULT_SEED = 42\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"catalog\", DEFAULT_CATALOG, \"Catalog\")\n",
        "    dbutils.widgets.text(\"schema_ref\", DEFAULT_SCHEMA_REF, \"Schema (ref)\")\n",
        "    dbutils.widgets.text(\"schema_bronze\", DEFAULT_SCHEMA_BRONZE, \"Schema (bronze)\")\n",
        "    dbutils.widgets.text(\"schema_silver\", DEFAULT_SCHEMA_SILVER, \"Schema (silver)\")\n",
        "    dbutils.widgets.text(\"schema_gold\", DEFAULT_SCHEMA_GOLD, \"Schema (gold)\")\n",
        "\n",
        "    dbutils.widgets.text(\"n_providers\", str(DEFAULT_N_PROVIDERS), \"N providers\")\n",
        "    dbutils.widgets.text(\"days_schedule\", str(DEFAULT_DAYS_SCHEDULE), \"Days schedule\")\n",
        "    dbutils.widgets.text(\"seed\", str(DEFAULT_SEED), \"Random seed\")\n",
        "\n",
        "    catalog = dbutils.widgets.get(\"catalog\") or DEFAULT_CATALOG\n",
        "    schema_ref = dbutils.widgets.get(\"schema_ref\") or DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = dbutils.widgets.get(\"schema_bronze\") or DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = dbutils.widgets.get(\"schema_silver\") or DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = dbutils.widgets.get(\"schema_gold\") or DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = int(dbutils.widgets.get(\"n_providers\") or DEFAULT_N_PROVIDERS)\n",
        "    DAYS_SCHEDULE = int(dbutils.widgets.get(\"days_schedule\") or DEFAULT_DAYS_SCHEDULE)\n",
        "    SEED = int(dbutils.widgets.get(\"seed\") or DEFAULT_SEED)\n",
        "except Exception:\n",
        "    catalog = DEFAULT_CATALOG\n",
        "    schema_ref = DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = DEFAULT_N_PROVIDERS\n",
        "    DAYS_SCHEDULE = DEFAULT_DAYS_SCHEDULE\n",
        "    SEED = DEFAULT_SEED\n",
        "\n",
        "# Derived helpers\n",
        "fq = lambda sch, tbl: f\"{catalog}.{sch}.{tbl}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog bootstrap (you may need permissions to create catalogs/schemas)\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "spark.sql(f\"USE CATALOG {catalog}\")\n",
        "for sch in [schema_ref, schema_bronze, schema_silver, schema_gold]:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{sch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.shift_raw\n",
        "Shift columns: (shift_id, facility_id, start_ts, end_ts, required_procedure_code, required_count, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "random.seed(SEED)\n",
        "base_ts = datetime(2026, 1, 1, 8, 0, 0)\n",
        "\n",
        "facility_ids = [r[\"facility_id\"] for r in spark.read.table(fq(schema_ref, \"facility\")).select(\"facility_id\").collect()]\n",
        "procedure_codes = [r[\"procedure_code\"] for r in spark.read.table(fq(schema_ref, \"procedure\")).select(\"procedure_code\").collect()]\n",
        "\n",
        "shift_rows = []\n",
        "shift_i = 0\n",
        "for day in range(DAYS_SCHEDULE):\n",
        "    day_start = base_ts + timedelta(days=day)\n",
        "    for fac in facility_ids:\n",
        "        for block in [0, 1]:\n",
        "            start_ts = day_start + timedelta(hours=block * 12)\n",
        "            end_ts = start_ts + timedelta(hours=12)\n",
        "            required_procedure_code = random.choice(procedure_codes)\n",
        "            required_count = random.randint(1, 3)\n",
        "            shift_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"shift-{SEED}-{shift_i}\"))\n",
        "            ingested_at = base_ts + timedelta(seconds=shift_i)\n",
        "            shift_rows.append((shift_id, fac, start_ts, end_ts, required_procedure_code, required_count, ingested_at))\n",
        "            shift_i += 1\n",
        "\n",
        "shift_schema = StructType([\n",
        "    StructField(\"shift_id\", StringType(), False),\n",
        "    StructField(\"facility_id\", StringType(), False),\n",
        "    StructField(\"start_ts\", TimestampType(), False),\n",
        "    StructField(\"end_ts\", TimestampType(), False),\n",
        "    StructField(\"required_procedure_code\", StringType(), False),\n",
        "    StructField(\"required_count\", IntegerType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "shift_df = spark.createDataFrame(shift_rows, shift_schema)\n",
        "shift_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"shift_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.assignment_raw\n",
        "Assignment columns: (assignment_id, shift_id, provider_id, assigned_ts, assignment_status, source_system, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "providers = [r[\"provider_id\"] for r in spark.read.table(fq(schema_bronze, \"provider_raw\")).select(\"provider_id\").collect()]\n",
        "\n",
        "assignment_rows = []\n",
        "as_i = 0\n",
        "for s in shift_rows:\n",
        "    shift_id, facility_id, start_ts, end_ts, proc_code, required_count, _ing = s\n",
        "    # Intentional gaps: assign between 0 and required_count providers\n",
        "    fill_n = random.randint(0, required_count)\n",
        "    chosen = random.sample(providers, k=min(fill_n, len(providers)))\n",
        "    for pid in chosen:\n",
        "        assignment_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"assign-{SEED}-{as_i}\"))\n",
        "        assigned_ts = start_ts - timedelta(hours=random.randint(1, 48))\n",
        "        assignment_status = random.choice([\"ASSIGNED\", \"ASSIGNED\", \"CANCELED\"])\n",
        "        source_system = random.choice([\"SCHED_SYS_A\", \"SCHED_SYS_B\"])\n",
        "        ingested_at = start_ts - timedelta(hours=1) + timedelta(seconds=as_i)\n",
        "        assignment_rows.append((assignment_id, shift_id, pid, assigned_ts, assignment_status, source_system, ingested_at))\n",
        "        as_i += 1\n",
        "\n",
        "assign_schema = StructType([\n",
        "    StructField(\"assignment_id\", StringType(), False),\n",
        "    StructField(\"shift_id\", StringType(), False),\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"assigned_ts\", TimestampType(), False),\n",
        "    StructField(\"assignment_status\", StringType(), False),\n",
        "    StructField(\"source_system\", StringType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "assign_df = spark.createDataFrame(assignment_rows, assign_schema)\n",
        "assign_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"assignment_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate\n",
        "Counts for both tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for t in [\"shift_raw\", \"assignment_raw\"]:\n",
        "    print(f\"{fq(schema_bronze, t)}: {spark.read.table(fq(schema_bronze, t)).count():,}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
