{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 â€” Generate shifts + assignments (bronze)\n",
        "\n",
        "Creates upcoming shifts for the next `DAYS_SCHEDULE` days and assignments with intentional gaps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install faker==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (Databricks widgets)\n",
        "# These widgets make the demo portable across workspaces/accounts.\n",
        "# If you're running this outside a Databricks notebook, it will fall back to defaults.\n",
        "\n",
        "DEFAULT_CATALOG = \"staffing_catalog\"\n",
        "DEFAULT_SCHEMA_REF = \"credentialing_ref\"\n",
        "DEFAULT_SCHEMA_BRONZE = \"credentialing_bronze\"\n",
        "DEFAULT_SCHEMA_SILVER = \"credentialing_silver\"\n",
        "DEFAULT_SCHEMA_GOLD = \"credentialing_gold\"\n",
        "\n",
        "DEFAULT_N_PROVIDERS = 200\n",
        "DEFAULT_DAYS_SCHEDULE = 14\n",
        "DEFAULT_SEED = 42\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"catalog\", DEFAULT_CATALOG, \"Catalog\")\n",
        "    dbutils.widgets.text(\"schema_ref\", DEFAULT_SCHEMA_REF, \"Schema (ref)\")\n",
        "    dbutils.widgets.text(\"schema_bronze\", DEFAULT_SCHEMA_BRONZE, \"Schema (bronze)\")\n",
        "    dbutils.widgets.text(\"schema_silver\", DEFAULT_SCHEMA_SILVER, \"Schema (silver)\")\n",
        "    dbutils.widgets.text(\"schema_gold\", DEFAULT_SCHEMA_GOLD, \"Schema (gold)\")\n",
        "\n",
        "    dbutils.widgets.text(\"n_providers\", str(DEFAULT_N_PROVIDERS), \"N providers\")\n",
        "    dbutils.widgets.text(\"days_schedule\", str(DEFAULT_DAYS_SCHEDULE), \"Days schedule\")\n",
        "    dbutils.widgets.text(\"seed\", str(DEFAULT_SEED), \"Random seed\")\n",
        "\n",
        "    catalog = dbutils.widgets.get(\"catalog\") or DEFAULT_CATALOG\n",
        "    schema_ref = dbutils.widgets.get(\"schema_ref\") or DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = dbutils.widgets.get(\"schema_bronze\") or DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = dbutils.widgets.get(\"schema_silver\") or DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = dbutils.widgets.get(\"schema_gold\") or DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = int(dbutils.widgets.get(\"n_providers\") or DEFAULT_N_PROVIDERS)\n",
        "    DAYS_SCHEDULE = int(dbutils.widgets.get(\"days_schedule\") or DEFAULT_DAYS_SCHEDULE)\n",
        "    SEED = int(dbutils.widgets.get(\"seed\") or DEFAULT_SEED)\n",
        "except Exception:\n",
        "    catalog = DEFAULT_CATALOG\n",
        "    schema_ref = DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = DEFAULT_N_PROVIDERS\n",
        "    DAYS_SCHEDULE = DEFAULT_DAYS_SCHEDULE\n",
        "    SEED = DEFAULT_SEED\n",
        "\n",
        "# Derived helpers\n",
        "fq = lambda sch, tbl: f\"{catalog}.{sch}.{tbl}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog bootstrap (you may need permissions to create catalogs/schemas)\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "spark.sql(f\"USE CATALOG {catalog}\")\n",
        "for sch in [schema_ref, schema_bronze, schema_silver, schema_gold]:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{sch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.shift_raw\n",
        "Shift columns: (shift_id, facility_id, start_ts, end_ts, required_procedure_code, required_count, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "import random\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "random.seed(SEED)\n",
        "base_ts = datetime(2026, 1, 1, 8, 0, 0)\n",
        "\n",
        "facility_ids = [r[\"facility_id\"] for r in spark.read.table(fq(schema_ref, \"facility\")).select(\"facility_id\").collect()]\n",
        "procedure_codes = [r[\"procedure_code\"] for r in spark.read.table(fq(schema_ref, \"procedure\")).select(\"procedure_code\").collect()]\n",
        "\n",
        "shift_rows = []\n",
        "shift_i = 0\n",
        "for day in range(DAYS_SCHEDULE):\n",
        "    day_start = base_ts + timedelta(days=day)\n",
        "    for fac in facility_ids:\n",
        "        for block in [0, 1]:\n",
        "            start_ts = day_start + timedelta(hours=block * 12)\n",
        "            end_ts = start_ts + timedelta(hours=12)\n",
        "            required_procedure_code = random.choice(procedure_codes)\n",
        "            required_count = random.randint(1, 3)\n",
        "            shift_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"shift-{SEED}-{shift_i}\"))\n",
        "            ingested_at = base_ts + timedelta(seconds=shift_i)\n",
        "            shift_rows.append((shift_id, fac, start_ts, end_ts, required_procedure_code, required_count, ingested_at))\n",
        "            shift_i += 1\n",
        "\n",
        "shift_schema = StructType([\n",
        "    StructField(\"shift_id\", StringType(), False),\n",
        "    StructField(\"facility_id\", StringType(), False),\n",
        "    StructField(\"start_ts\", TimestampType(), False),\n",
        "    StructField(\"end_ts\", TimestampType(), False),\n",
        "    StructField(\"required_procedure_code\", StringType(), False),\n",
        "    StructField(\"required_count\", IntegerType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "shift_df = spark.createDataFrame(shift_rows, shift_schema)\n",
        "shift_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"shift_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.assignment_raw\n",
        "Assignment columns: (assignment_id, shift_id, provider_id, assigned_ts, assignment_status, source_system, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "providers = [r[\"provider_id\"] for r in spark.read.table(fq(schema_bronze, \"provider_raw\")).select(\"provider_id\").collect()]\n",
        "\n",
        "assignment_rows = []\n",
        "as_i = 0\n",
        "for s in shift_rows:\n",
        "    shift_id, facility_id, start_ts, end_ts, proc_code, required_count, _ing = s\n",
        "    # Intentional gaps: assign between 0 and required_count providers\n",
        "    fill_n = random.randint(0, required_count)\n",
        "    chosen = random.sample(providers, k=min(fill_n, len(providers)))\n",
        "    for pid in chosen:\n",
        "        assignment_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"assign-{SEED}-{as_i}\"))\n",
        "        assigned_ts = start_ts - timedelta(hours=random.randint(1, 48))\n",
        "        assignment_status = random.choice([\"ASSIGNED\", \"ASSIGNED\", \"CANCELED\"])\n",
        "        source_system = random.choice([\"SCHED_SYS_A\", \"SCHED_SYS_B\"])\n",
        "        ingested_at = start_ts - timedelta(hours=1) + timedelta(seconds=as_i)\n",
        "        assignment_rows.append((assignment_id, shift_id, pid, assigned_ts, assignment_status, source_system, ingested_at))\n",
        "        as_i += 1\n",
        "\n",
        "assign_schema = StructType([\n",
        "    StructField(\"assignment_id\", StringType(), False),\n",
        "    StructField(\"shift_id\", StringType(), False),\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"assigned_ts\", TimestampType(), False),\n",
        "    StructField(\"assignment_status\", StringType(), False),\n",
        "    StructField(\"source_system\", StringType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "assign_df = spark.createDataFrame(assignment_rows, assign_schema)\n",
        "assign_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"assignment_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bronze.nurse_assignment_raw\n",
        "Nurse assignments by unit for staffing optimization: (assignment_id, unit_id, provider_id, assignment_date, shift_start, shift_end, assignment_status, source_system, ingested_at)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate nurse assignments per unit for nurse staffing optimization\n",
        "unit_ids = [r[\"unit_id\"] for r in spark.read.table(fq(schema_ref, \"unit\")).select(\"unit_id\").collect()]\n",
        "units_df = spark.read.table(fq(schema_ref, \"unit\")).collect()\n",
        "units_map = {r[\"unit_id\"]: {\"bed_count\": r[\"bed_count\"], \"target_ratio\": r[\"target_ratio\"], \"facility_id\": r[\"facility_id\"]} for r in units_df}\n",
        "\n",
        "# Filter providers to those with a primary unit (nurses)\n",
        "nurse_providers = [(r[\"provider_id\"], r[\"primary_unit_id\"], r[\"employment_type\"]) \n",
        "                   for r in spark.read.table(fq(schema_bronze, \"provider_raw\"))\n",
        "                   .filter(\"primary_unit_id IS NOT NULL\")\n",
        "                   .select(\"provider_id\", \"primary_unit_id\", \"employment_type\").collect()]\n",
        "\n",
        "nurse_assignment_rows = []\n",
        "na_i = 0\n",
        "\n",
        "for day in range(DAYS_SCHEDULE):\n",
        "    assignment_date = (base_ts + timedelta(days=day)).date()\n",
        "    \n",
        "    for unit_id in unit_ids:\n",
        "        unit_info = units_map.get(unit_id, {\"bed_count\": 20, \"target_ratio\": 4.0})\n",
        "        # Simulate census: 60-90% of bed count\n",
        "        census = int(unit_info[\"bed_count\"] * random.uniform(0.6, 0.9))\n",
        "        nurses_needed = max(1, int(census / unit_info[\"target_ratio\"]))\n",
        "        \n",
        "        # Find nurses assigned to this unit (or float from elsewhere)\n",
        "        unit_nurses = [p for p in nurse_providers if p[1] == unit_id]\n",
        "        float_nurses = [p for p in nurse_providers if p[1] != unit_id]\n",
        "        \n",
        "        # Assign nurses: some understaffed, some optimal, some overstaffed for demo variety\n",
        "        variance = random.choice([-2, -1, 0, 0, 0, 1, 2])\n",
        "        nurses_to_assign = max(1, nurses_needed + variance)\n",
        "        \n",
        "        # Prioritize unit nurses, then float\n",
        "        available = unit_nurses[:] + random.sample(float_nurses, min(len(float_nurses), 10))\n",
        "        chosen = random.sample(available, min(nurses_to_assign, len(available)))\n",
        "        \n",
        "        for nurse in chosen:\n",
        "            assignment_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f\"nurse-assign-{SEED}-{na_i}\"))\n",
        "            shift_start = datetime.combine(assignment_date, datetime.min.time()) + timedelta(hours=7)  # 7am start\n",
        "            shift_end = shift_start + timedelta(hours=12)  # 12-hour shift\n",
        "            assignment_status = \"ASSIGNED\" if random.random() > 0.05 else \"CANCELED\"\n",
        "            source_system = random.choice([\"STAFFING_SYS_A\", \"STAFFING_SYS_B\"])\n",
        "            ingested_at = base_ts + timedelta(seconds=300000 + na_i)\n",
        "            \n",
        "            nurse_assignment_rows.append((\n",
        "                assignment_id, unit_id, nurse[0], assignment_date, shift_start, shift_end,\n",
        "                assignment_status, source_system, ingested_at\n",
        "            ))\n",
        "            na_i += 1\n",
        "\n",
        "nurse_assign_schema = StructType([\n",
        "    StructField(\"assignment_id\", StringType(), False),\n",
        "    StructField(\"unit_id\", StringType(), False),\n",
        "    StructField(\"provider_id\", StringType(), False),\n",
        "    StructField(\"assignment_date\", DateType(), False),\n",
        "    StructField(\"shift_start\", TimestampType(), False),\n",
        "    StructField(\"shift_end\", TimestampType(), False),\n",
        "    StructField(\"assignment_status\", StringType(), False),\n",
        "    StructField(\"source_system\", StringType(), False),\n",
        "    StructField(\"ingested_at\", TimestampType(), False),\n",
        "])\n",
        "\n",
        "nurse_assign_df = spark.createDataFrame(nurse_assignment_rows, nurse_assign_schema)\n",
        "nurse_assign_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fq(schema_bronze, \"nurse_assignment_raw\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate\n",
        "Counts for all tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for t in [\"shift_raw\", \"assignment_raw\", \"nurse_assignment_raw\"]:\n",
        "    print(f\"{fq(schema_bronze, t)}: {spark.read.table(fq(schema_bronze, t)).count():,}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
