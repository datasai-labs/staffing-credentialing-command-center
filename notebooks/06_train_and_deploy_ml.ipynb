{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 — Train + deploy ML model (gap risk) with MLflow + Unity Catalog\n",
        "\n",
        "Trains a shift-level model to predict whether a shift will have a staffing gap, registers it to Unity Catalog Model Registry, and writes batch predictions back to gold tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install mlflow==2.12.2 databricks-sdk==0.28.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (Databricks widgets)\n",
        "# These widgets make the demo portable across workspaces/accounts.\n",
        "# If you're running this outside a Databricks notebook, it will fall back to defaults.\n",
        "\n",
        "DEFAULT_CATALOG = \"rtpa_catalog\"\n",
        "DEFAULT_SCHEMA_REF = \"credentialing_ref\"\n",
        "DEFAULT_SCHEMA_BRONZE = \"credentialing_bronze\"\n",
        "DEFAULT_SCHEMA_SILVER = \"credentialing_silver\"\n",
        "DEFAULT_SCHEMA_GOLD = \"credentialing_gold\"\n",
        "\n",
        "DEFAULT_N_PROVIDERS = 200\n",
        "DEFAULT_DAYS_SCHEDULE = 14\n",
        "DEFAULT_SEED = 42\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"catalog\", DEFAULT_CATALOG, \"Catalog\")\n",
        "    dbutils.widgets.text(\"schema_ref\", DEFAULT_SCHEMA_REF, \"Schema (ref)\")\n",
        "    dbutils.widgets.text(\"schema_bronze\", DEFAULT_SCHEMA_BRONZE, \"Schema (bronze)\")\n",
        "    dbutils.widgets.text(\"schema_silver\", DEFAULT_SCHEMA_SILVER, \"Schema (silver)\")\n",
        "    dbutils.widgets.text(\"schema_gold\", DEFAULT_SCHEMA_GOLD, \"Schema (gold)\")\n",
        "\n",
        "    dbutils.widgets.text(\"n_providers\", str(DEFAULT_N_PROVIDERS), \"N providers\")\n",
        "    dbutils.widgets.text(\"days_schedule\", str(DEFAULT_DAYS_SCHEDULE), \"Days schedule\")\n",
        "    dbutils.widgets.text(\"seed\", str(DEFAULT_SEED), \"Random seed\")\n",
        "\n",
        "    catalog = dbutils.widgets.get(\"catalog\") or DEFAULT_CATALOG\n",
        "    schema_ref = dbutils.widgets.get(\"schema_ref\") or DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = dbutils.widgets.get(\"schema_bronze\") or DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = dbutils.widgets.get(\"schema_silver\") or DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = dbutils.widgets.get(\"schema_gold\") or DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = int(dbutils.widgets.get(\"n_providers\") or DEFAULT_N_PROVIDERS)\n",
        "    DAYS_SCHEDULE = int(dbutils.widgets.get(\"days_schedule\") or DEFAULT_DAYS_SCHEDULE)\n",
        "    SEED = int(dbutils.widgets.get(\"seed\") or DEFAULT_SEED)\n",
        "except Exception:\n",
        "    catalog = DEFAULT_CATALOG\n",
        "    schema_ref = DEFAULT_SCHEMA_REF\n",
        "    schema_bronze = DEFAULT_SCHEMA_BRONZE\n",
        "    schema_silver = DEFAULT_SCHEMA_SILVER\n",
        "    schema_gold = DEFAULT_SCHEMA_GOLD\n",
        "\n",
        "    N_PROVIDERS = DEFAULT_N_PROVIDERS\n",
        "    DAYS_SCHEDULE = DEFAULT_DAYS_SCHEDULE\n",
        "    SEED = DEFAULT_SEED\n",
        "\n",
        "# Derived helpers\n",
        "fq = lambda sch, tbl: f\"{catalog}.{sch}.{tbl}\"\n",
        "\n",
        "# Model registry name (Unity Catalog 3-level name: <catalog>.<schema>.<model>)\n",
        "MODEL_NAME = f\"{catalog}.{schema_gold}.shift_gap_risk_model\"\n",
        "\n",
        "# Where we publish predictions\n",
        "PRED_TABLE = fq(schema_gold, \"shift_gap_predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unity Catalog bootstrap (you may need permissions to create catalogs/schemas)\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "spark.sql(f\"USE CATALOG {catalog}\")\n",
        "for sch in [schema_ref, schema_bronze, schema_silver, schema_gold]:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{sch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build training set\n",
        "\n",
        "We train on `gold.staffing_gaps` (label = gap_count > 0) plus joined reference attributes, with time-derived features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "gaps = spark.read.table(fq(schema_gold, \"staffing_gaps\"))\n",
        "ref_proc = spark.read.table(fq(schema_ref, \"procedure\")).select(\n",
        "    F.col(\"procedure_code\").alias(\"required_procedure_code\"),\n",
        "    \"requires_privilege\",\n",
        "    \"requires_acls\"\n",
        ")\n",
        "\n",
        "base = (\n",
        "    gaps\n",
        "      .join(ref_proc, \"required_procedure_code\", \"left\")\n",
        "      .withColumn(\"shift_date\", F.to_date(\"start_ts\"))\n",
        "      .withColumn(\"dow\", F.dayofweek(\"start_ts\"))\n",
        "      .withColumn(\"hour\", F.hour(\"start_ts\"))\n",
        "      .withColumn(\"is_weekend\", F.when(F.col(\"dow\").isin([1, 7]), F.lit(1)).otherwise(F.lit(0)))\n",
        "      .withColumn(\"days_to_shift\", F.datediff(F.to_date(\"start_ts\"), F.current_date()))\n",
        "      .withColumn(\"label\", F.when(F.col(\"gap_count\") > 0, F.lit(1)).otherwise(F.lit(0)))\n",
        ")\n",
        "\n",
        "# Basic sanity\n",
        "display(base.select(\"shift_id\", \"facility_id\", \"required_procedure_code\", \"required_count\", \"assigned_count\", \"eligible_provider_count\", \"gap_count\", \"label\").limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a Spark ML pipeline\n",
        "\n",
        "Uses categorical encodings + a classifier, logs to MLflow, and registers the model to Unity Catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import mlflow\n",
        "import mlflow.spark\n",
        "\n",
        "# Ensure we use Unity Catalog model registry\n",
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "# Keep only rows with the minimal required fields\n",
        "data = (\n",
        "    base\n",
        "      .select(\n",
        "          \"label\",\n",
        "          \"facility_id\",\n",
        "          \"required_procedure_code\",\n",
        "          \"required_count\",\n",
        "          \"assigned_count\",\n",
        "          \"eligible_provider_count\",\n",
        "          \"requires_privilege\",\n",
        "          \"requires_acls\",\n",
        "          \"dow\",\n",
        "          \"hour\",\n",
        "          \"is_weekend\",\n",
        "          \"days_to_shift\",\n",
        "          \"shift_id\",\n",
        "          \"shift_date\"\n",
        "      )\n",
        "      .fillna({\"requires_privilege\": False, \"requires_acls\": False})\n",
        "      .withColumn(\"requires_privilege_i\", F.col(\"requires_privilege\").cast(\"int\"))\n",
        "      .withColumn(\"requires_acls_i\", F.col(\"requires_acls\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "train_df, test_df = data.randomSplit([0.8, 0.2], seed=SEED)\n",
        "\n",
        "# Categorical\n",
        "facility_indexer = StringIndexer(inputCol=\"facility_id\", outputCol=\"facility_id_idx\", handleInvalid=\"keep\")\n",
        "proc_indexer = StringIndexer(inputCol=\"required_procedure_code\", outputCol=\"proc_code_idx\", handleInvalid=\"keep\")\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=[\"facility_id_idx\", \"proc_code_idx\"],\n",
        "    outputCols=[\"facility_ohe\", \"proc_ohe\"]\n",
        ")\n",
        "\n",
        "# Numeric\n",
        "numeric_cols = [\n",
        "    \"required_count\",\n",
        "    \"assigned_count\",\n",
        "    \"eligible_provider_count\",\n",
        "    \"requires_privilege_i\",\n",
        "    \"requires_acls_i\",\n",
        "    \"dow\",\n",
        "    \"hour\",\n",
        "    \"is_weekend\",\n",
        "    \"days_to_shift\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"facility_ohe\", \"proc_ohe\"] + numeric_cols,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Classifier: Logistic regression (fast, interpretable)\n",
        "clf = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=50,\n",
        "    regParam=0.1,\n",
        "    elasticNetParam=0.0\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[facility_indexer, proc_indexer, encoder, assembler, clf])\n",
        "\n",
        "mlflow.spark.autolog()\n",
        "\n",
        "with mlflow.start_run(run_name=\"shift_gap_risk_train\"):\n",
        "    model = pipeline.fit(train_df)\n",
        "    pred = model.transform(test_df)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "    auc = evaluator.evaluate(pred)\n",
        "\n",
        "    # Basic accuracy\n",
        "    scored = pred.select(\"label\", F.col(\"prediction\").cast(\"int\").alias(\"pred\"))\n",
        "    acc = scored.filter(F.col(\"label\") == F.col(\"pred\")).count() / max(1, scored.count())\n",
        "\n",
        "    mlflow.log_metric(\"test_auc\", float(auc))\n",
        "    mlflow.log_metric(\"test_accuracy\", float(acc))\n",
        "\n",
        "    # Log + register to Unity Catalog (3-level name)\n",
        "    mlflow.spark.log_model(\n",
        "        spark_model=model,\n",
        "        artifact_path=\"model\",\n",
        "        registered_model_name=MODEL_NAME\n",
        "    )\n",
        "\n",
        "print(f\"AUC={auc:.4f}, accuracy={acc:.4f}\")\n",
        "display(pred.select(\"label\", \"prediction\", \"probability\").limit(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set a ‘Champion’ alias (optional)\n",
        "\n",
        "This makes it easier to deploy consistently (you can reference the model by alias in downstream jobs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "client = MlflowClient()\n",
        "\n",
        "# Get the newest version (by version number)\n",
        "versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
        "latest = max([int(v.version) for v in versions]) if versions else None\n",
        "print(f\"Latest registered version for {MODEL_NAME}: {latest}\")\n",
        "\n",
        "# Set alias if supported / permitted\n",
        "if latest is not None:\n",
        "    try:\n",
        "        client.set_registered_model_alias(MODEL_NAME, \"Champion\", str(latest))\n",
        "        print(\"Alias set: Champion\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not set alias (permissions or feature availability). Continuing.\")\n",
        "        print(str(e)[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch deployment: write predictions table\n",
        "\n",
        "Loads the registered model and scores all shifts in `gold.staffing_gaps`, writing results to `gold.shift_gap_predictions` for dashboards/apps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose a stable model URI: prefer alias if available, else use latest version\n",
        "model_uri = f\"models:/{MODEL_NAME}@Champion\"\n",
        "\n",
        "try:\n",
        "    loaded = mlflow.spark.load_model(model_uri)\n",
        "    print(f\"Loaded model by alias: {model_uri}\")\n",
        "except Exception:\n",
        "    # Fall back to latest numeric version\n",
        "    versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
        "    latest = max([int(v.version) for v in versions])\n",
        "    model_uri = f\"models:/{MODEL_NAME}/{latest}\"\n",
        "    loaded = mlflow.spark.load_model(model_uri)\n",
        "    print(f\"Loaded model by version: {model_uri}\")\n",
        "\n",
        "# Rebuild the same feature columns used for training\n",
        "to_score = (\n",
        "    spark.read.table(fq(schema_gold, \"staffing_gaps\"))\n",
        "      .join(\n",
        "          spark.read.table(fq(schema_ref, \"procedure\")).select(\n",
        "              F.col(\"procedure_code\").alias(\"required_procedure_code\"),\n",
        "              \"requires_privilege\",\n",
        "              \"requires_acls\"\n",
        "          ),\n",
        "          \"required_procedure_code\",\n",
        "          \"left\"\n",
        "      )\n",
        "      .withColumn(\"shift_date\", F.to_date(\"start_ts\"))\n",
        "      .withColumn(\"dow\", F.dayofweek(\"start_ts\"))\n",
        "      .withColumn(\"hour\", F.hour(\"start_ts\"))\n",
        "      .withColumn(\"is_weekend\", F.when(F.col(\"dow\").isin([1, 7]), F.lit(1)).otherwise(F.lit(0)))\n",
        "      .withColumn(\"days_to_shift\", F.datediff(F.to_date(\"start_ts\"), F.current_date()))\n",
        "      .fillna({\"requires_privilege\": False, \"requires_acls\": False})\n",
        "      .withColumn(\"requires_privilege_i\", F.col(\"requires_privilege\").cast(\"int\"))\n",
        "      .withColumn(\"requires_acls_i\", F.col(\"requires_acls\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "# The logged Spark ML pipeline expects the same raw columns (it includes indexers/encoders/assembler)\n",
        "scored = loaded.transform(\n",
        "    to_score.select(\n",
        "        \"facility_id\",\n",
        "        \"required_procedure_code\",\n",
        "        \"required_count\",\n",
        "        \"assigned_count\",\n",
        "        \"eligible_provider_count\",\n",
        "        \"requires_privilege_i\",\n",
        "        \"requires_acls_i\",\n",
        "        \"dow\",\n",
        "        \"hour\",\n",
        "        \"is_weekend\",\n",
        "        \"days_to_shift\",\n",
        "        \"shift_id\",\n",
        "        \"shift_date\",\n",
        "        \"gap_count\",\n",
        "        \"risk_level\",\n",
        "        \"risk_reason\"\n",
        "    ).withColumnRenamed(\"requires_privilege_i\", \"requires_privilege\")\n",
        "     .withColumnRenamed(\"requires_acls_i\", \"requires_acls\")\n",
        ")\n",
        "\n",
        "# Extract probability of class 1 (gap)\n",
        "pred_out = (\n",
        "    scored\n",
        "      .withColumn(\"predicted_is_gap\", F.col(\"prediction\").cast(\"int\"))\n",
        "      .withColumn(\"predicted_gap_prob\", F.col(\"probability\").getItem(1))\n",
        "      .withColumn(\"scored_at\", F.current_timestamp())\n",
        "      .select(\n",
        "          \"shift_id\",\n",
        "          \"shift_date\",\n",
        "          \"facility_id\",\n",
        "          \"required_procedure_code\",\n",
        "          \"required_count\",\n",
        "          \"assigned_count\",\n",
        "          \"eligible_provider_count\",\n",
        "          \"gap_count\",\n",
        "          \"risk_level\",\n",
        "          \"risk_reason\",\n",
        "          \"predicted_is_gap\",\n",
        "          \"predicted_gap_prob\",\n",
        "          \"scored_at\"\n",
        "      )\n",
        ")\n",
        "\n",
        "pred_out.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(PRED_TABLE)\n",
        "print(f\"Wrote predictions: {PRED_TABLE}\")\n",
        "display(pred_out.orderBy(F.desc(\"predicted_gap_prob\")).limit(25))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: create a Model Serving endpoint\n",
        "\n",
        "If you have permissions, you can deploy the registered model as a real-time endpoint using the Databricks deployments client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DO_CREATE_SERVING_ENDPOINT = False\n",
        "SERVING_ENDPOINT_NAME = \"shift-gap-risk-endpoint\"\n",
        "\n",
        "if DO_CREATE_SERVING_ENDPOINT:\n",
        "    import mlflow\n",
        "    from mlflow.deployments import get_deploy_client\n",
        "\n",
        "    mlflow.set_registry_uri(\"databricks-uc\")\n",
        "    client = get_deploy_client(\"databricks\")\n",
        "\n",
        "    # Note: config details vary by workspace; this is a minimal example.\n",
        "    # Requires permissions for model serving.\n",
        "    endpoint = client.create_endpoint(\n",
        "        name=SERVING_ENDPOINT_NAME,\n",
        "        config={\n",
        "            \"served_entities\": [\n",
        "                {\n",
        "                    \"entity_name\": MODEL_NAME,\n",
        "                    \"entity_version\": str(latest) if latest is not None else \"1\",\n",
        "                    \"workload_size\": \"Small\",\n",
        "                    \"scale_to_zero_enabled\": True\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "    print(endpoint)\n",
        "else:\n",
        "    print(\"Skipping endpoint creation. Set DO_CREATE_SERVING_ENDPOINT=True to attempt deployment.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
